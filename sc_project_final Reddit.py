# -*- coding: utf-8 -*-
"""SC project Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18M9JPMGhTzCvPY-qmJYaV2Msp8kv1OH8
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import joblib
from sklearn.feature_extraction.text import CountVectorizer

#Reading data from reddit dataset
data = pd.read_csv('/content/Reddit_Data.xls')
#df = DataFrame()
data['clean_comment'] = data['clean_comment'].str.strip().str.lower()
#df = pd.DataFrame(data)
data = data.fillna('')

data.head(20)

# Split into training and testing data
x = data['clean_comment']
y = data['category']
#x, y = df.CONTENT.fillna(' '), df.sentiment
x, x_test, y, y_test = train_test_split(x,y,test_size=0.33)

print (x)

# Vectorizing text reviews to numbers
vec = CountVectorizer(stop_words='english')
x = vec.fit_transform(x).toarray()
x_test = vec.transform(x_test).toarray()

#Implementing the model accuracy for Naive Bayes 
#Finalized this model
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(x_test,y_test)

model.score(x_test,y_test)

#Implementing the model accuracy for RandomForestClassifier
#To compare the accuracy with previous Naive-Bayes Model
#from sklearn.ensemble import RandomForestClassifier
#rfc=RandomForestClassifier()
#rfc.fit(x_test,y_test)

#rfc.score(x_test, y_test)
#Accuracy: 0.9991866

#Implementing the model accuracy for Logistic Regression
#To compare the accuracy with previous RandomForestClassifier
#from sklearn.linear_model import LogisticRegression
#reg = LogisticRegression()
#reg.fit(x_test,y_test)

#reg.score(x_test,y_test)
#accuracy: 0.97250

#Implementing the model accuracy for Adaboostclassifier
#To compare the accuracy with previous Logistic Regression Model
#from sklearn.ensemble import AdaBoostClassifier
#adb = AdaBoostClassifier(base_estimator = None)
#adb.fit(x_test,y_test)

#adb.score(x_test,y_test)
#accuracy: 0.688

model.predict(vec.transform(['This is amazing!']))

#Reddit credentials and data fetch
!pip install praw
import praw
import pandas as pd

# Define user agent
user_agent = "praw_scraper_1.0"

# Create an instance of reddit class
reddit = praw.Reddit(username="shahanasherfudeen",
                     password="99Shana99",
                     client_id="UVVmqCC0YddyVf446sfYeg",
                     client_secret="IKurXNLHSMRiGUS2Y-Z62X6ZBdihpw",
                     user_agent=user_agent
)

# Create sub-reddit instance
subreddit_name = "alexa"
subreddit = reddit.subreddit(subreddit_name)

df = pd.DataFrame() # creating dataframe for displaying scraped data

#print(subreddit)
# creating lists for storing scraped data
titles=[]

# looping over posts and scraping it
for submission in subreddit.top(limit=500):
    titles.append(submission.title)
    
    
df['Title'] = titles

print(titles)
#print(df.shape)
#df.head(300)

#Calculating Postive, negative, neutral count
pos_count= 0
neg_count= 0
neutral_count= 0

for text in titles:
  #print(text)
  p = model.predict(vec.transform([text]))
  if p == 1:
    pos_count = pos_count + 1
  elif p == -1:
    neg_count = neg_count + 1
  else:
    neutral_count = neutral_count + 1

print("postive  =", pos_count)
print("negative =", neg_count)
print("neutral =",neutral_count)

# Create sub-reddit instance
subreddit_name = "siri"
subreddit = reddit.subreddit(subreddit_name)

df = pd.DataFrame() # creating dataframe for displaying scraped data

#print(subreddit)
# creating lists for storing scraped data
titles=[]

# looping over posts and scraping it
for submission in subreddit.top(limit=500):
    titles.append(submission.title)
    
    
df['Title'] = titles

print(titles)
#print(df.shape)
#df.head(300)

pos_count= 0
neg_count= 0
neutral_count= 0

for text in titles:
  #print(text)
  p = model.predict(vec.transform([text]))
  if p == 1:
    pos_count = pos_count + 1
  elif p == -1:
    neg_count = neg_count + 1
  else:
    neutral_count = neutral_count + 1

print("postive  =", pos_count)
print("negative =", neg_count)
print("neutral =",neutral_count)